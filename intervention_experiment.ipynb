{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test `TorchEqualityExperiment()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch_equality import *\n",
    "\n",
    "params = dict(\n",
    "    embed_dims=[10],\n",
    "    hidden_dims=[100],\n",
    "    alphas=[0.001],\n",
    "    learning_rates=[0.01],\n",
    "    n_trials=2,\n",
    "    train_sizes=list(range(1004, 2005, 1000))\n",
    ")\n",
    "\n",
    "experiment = TorchEqualityExperiment(dataset_class=TorchEqualityDataset, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid size: 1 * 2; 2 experiments\n",
      "Running trials for embed_dim=10 hidden_dim=100 alpha=0.001 lr=0.01 ... mean: 0.83; max: 0.998; took 6.0 secs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial</th>\n",
       "      <th>train_size</th>\n",
       "      <th>embed_dim</th>\n",
       "      <th>hidden_dim</th>\n",
       "      <th>alpha</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>batch_pos</th>\n",
       "      <th>batch_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1004</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.990</td>\n",
       "      <td>503</td>\n",
       "      <td>501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2004</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.996</td>\n",
       "      <td>1002</td>\n",
       "      <td>1002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.514</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1004</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.990</td>\n",
       "      <td>498</td>\n",
       "      <td>506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>2004</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.998</td>\n",
       "      <td>1002</td>\n",
       "      <td>1002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trial  train_size  embed_dim  hidden_dim  alpha  learning_rate  accuracy  \\\n",
       "0      1           0         10         100  0.001           0.01     0.476   \n",
       "1      1        1004         10         100  0.001           0.01     0.990   \n",
       "2      1        2004         10         100  0.001           0.01     0.996   \n",
       "3      2           0         10         100  0.001           0.01     0.514   \n",
       "4      2        1004         10         100  0.001           0.01     0.990   \n",
       "5      2        2004         10         100  0.001           0.01     0.998   \n",
       "\n",
       "   batch_pos  batch_neg  \n",
       "0          0          0  \n",
       "1        503        501  \n",
       "2       1002       1002  \n",
       "3          0          0  \n",
       "4        498        506  \n",
       "5       1002       1002  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model for intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "embed_dim = 10\n",
    "max_epochs = 100\n",
    "hidden_dim = 100\n",
    "train_size = 2000\n",
    "test_size = 500\n",
    "alpha = 0.001\n",
    "lr = 0.01\n",
    "\n",
    "train_dataset = TorchEqualityDataset(embed_dim=embed_dim, n_pos=train_size//2, n_neg=train_size//2)\n",
    "test_dataset = TorchEqualityDataset(embed_dim=embed_dim, n_pos=test_size//2, n_neg=test_size//2)\n",
    "\n",
    "train_dataset.test_disjoint(test_dataset)\n",
    "model = TorchEqualityModel(max_epochs=max_epochs,\n",
    "                           input_size=embed_dim*2,\n",
    "                           batch_size=1000,\n",
    "                           hidden_layer_size=hidden_dim,\n",
    "                           alpha=alpha,\n",
    "                           lr=lr,\n",
    "                           gpu=True)\n",
    "\n",
    "model.fit(train_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test `TorchEqualityIntervention`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.43706961 -0.4827849   0.37416355 -0.28851521  0.24289165  0.19498498\n",
      " -0.26960545 -0.21563691  0.25949174 -0.01998772 -0.26968687 -0.46189744\n",
      "  0.32713884 -0.13627835 -0.39339898  0.29566419  0.22301776 -0.27912615\n",
      "  0.18120715 -0.36316504] 0\n",
      "[ 0.34292536 -0.30845335 -0.39574798  0.09547243  0.45492058  0.1343681\n",
      "  0.20372263 -0.14990982 -0.4816382   0.05901701  0.17036996 -0.31355334\n",
      " -0.40446764 -0.2006948   0.15480174  0.26021808 -0.36356341 -0.09771308\n",
      "  0.20747919  0.25207756] 0\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset.X[0], test_dataset.y[0])\n",
    "print(test_dataset.X[3], test_dataset.y[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from intervention_interface import *\n",
    "\n",
    "intervention = TorchEqualityIntervention(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "res_3 = intervention.run(torch.tensor(test_dataset.X[3]))\n",
    "print(res_3)\n",
    "res_0 = intervention.run(torch.tensor(test_dataset.X[0]))\n",
    "print(res_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.7307e-02, 0.0000e+00, 0.0000e+00, 1.4745e-02, 1.1599e-01, 0.0000e+00,\n",
      "        0.0000e+00, 4.7905e-01, 2.8397e-01, 2.0213e-01, 3.2581e-01, 8.1380e-02,\n",
      "        4.0340e-03, 4.0865e-01, 3.2057e-01, 0.0000e+00, 2.0551e-01, 0.0000e+00,\n",
      "        8.4007e-01, 4.3933e-01, 5.4185e-02, 0.0000e+00, 1.0326e-01, 1.1600e-01,\n",
      "        0.0000e+00, 0.0000e+00, 1.0331e-01, 0.0000e+00, 1.1047e-01, 0.0000e+00,\n",
      "        1.7635e-01, 5.3663e-01, 1.0845e-01, 2.5078e-01, 9.1275e-02, 1.1915e-01,\n",
      "        6.5898e-01, 0.0000e+00, 0.0000e+00, 2.1667e-01, 0.0000e+00, 0.0000e+00,\n",
      "        9.7042e-02, 0.0000e+00, 4.2061e-02, 2.9561e-01, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 2.1148e-01, 3.2209e-01, 2.5021e-01, 0.0000e+00, 4.2517e-01,\n",
      "        3.0799e-01, 0.0000e+00, 0.0000e+00, 1.8936e-02, 3.5749e-01, 2.0232e-01,\n",
      "        0.0000e+00, 0.0000e+00, 1.7303e-01, 3.6376e-02, 7.9375e-02, 3.7546e-01,\n",
      "        9.3856e-02, 1.3494e-01, 0.0000e+00, 0.0000e+00, 3.3927e-01, 7.0746e-02,\n",
      "        0.0000e+00, 6.4800e-02, 2.5557e-01, 2.0887e-01, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 8.2156e-04, 0.0000e+00, 5.6295e-02, 1.0092e-01, 2.6266e-01,\n",
      "        1.3242e-01, 5.7675e-01, 1.2994e-01, 2.7564e-01, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 5.5776e-03, 1.2937e-01, 7.1164e-02, 0.0000e+00, 3.1612e-01,\n",
      "        3.9426e-01, 0.0000e+00, 4.3069e-02, 1.2948e-02], device='cuda:0')\n",
      "[0] [0]\n"
     ]
    }
   ],
   "source": [
    "res_3 = intervention.run(torch.tensor(test_dataset.X[3]))\n",
    "print(intervention.get_from_cache(\"hidden_vec\"))\n",
    "res_intervention = intervention.fix_and_run(\"hidden_vec\", torch.tensor(test_dataset.X[0]))\n",
    "print(res_3, res_intervention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test newest `ComputationGraph`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I've added a setup.py in the Interchange repo, so we can install the `intervention` package locally using\n",
    "#   $ pip install -e path/to/Interchange\n",
    "\n",
    "from intervention import ComputationGraph, GraphNode\n",
    "\n",
    "class TorchEqualityCompGraph(ComputationGraph):\n",
    "    def __init__(self, model):\n",
    "        assert isinstance(model, TorchEqualityModel)\n",
    "        self.model = model\n",
    "        self.module = model.module\n",
    "\n",
    "        @GraphNode()\n",
    "        def linear(x):\n",
    "            # preprocess inputs here\n",
    "            x = x.float().to(self.model.device)\n",
    "            return self.module.linear(x)\n",
    "\n",
    "        @GraphNode(linear)\n",
    "        def activation(x):\n",
    "            return self.module.activation(x)\n",
    "\n",
    "        @GraphNode(activation)\n",
    "        def logits(x):\n",
    "            return self.module.output(x)\n",
    "\n",
    "        @GraphNode(logits)\n",
    "        def root(x):\n",
    "            scores = self.module.sigmoid(x)\n",
    "            return [1 if z >= 0.5 else 0 for z in scores]\n",
    "\n",
    "        super().__init__(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intervene on entire tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res_0: [0] res_3: [0]\n",
      "input_0 before intervention [0], after intervention [0]\n"
     ]
    }
   ],
   "source": [
    "from intervention import GraphInput, Intervention\n",
    "\n",
    "g = TorchEqualityCompGraph(model)\n",
    "\n",
    "input_0 = GraphInput({\"linear\": torch.tensor(test_dataset.X[0])})\n",
    "input_3 = GraphInput({\"linear\": torch.tensor(test_dataset.X[3])})\n",
    "\n",
    "res_0 = g.compute(input_0)\n",
    "res_3 = g.compute(input_3)\n",
    "\n",
    "print(\"res_0:\", res_0, \"res_3:\", res_3)\n",
    "\n",
    "# Use input_0 as input, but set the result of \"activation\" node to that of input_3\n",
    "interv_3_0 = Intervention(input_0, {\"activation\": g.get_result(\"activation\", input_3)})\n",
    "\n",
    "before, after = g.intervene(interv_3_0)\n",
    "\n",
    "print(\"input_0 before intervention %s, after intervention %s\" % (before, after))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intervene on a part of a tensor\n",
    "\n",
    "We can specify which part of a tensor we would like to intervene by adding indexing after a node's name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_0 before intervention [0], after intervention [0]\n"
     ]
    }
   ],
   "source": [
    "in2 = Intervention(input_0)\n",
    "\n",
    "replace_value = g.get_result(\"activation\", input_3)[:10]\n",
    "inputs = {\"activation[:10]\": replace_value}\n",
    "in2.inputs = inputs\n",
    "\n",
    "before, after = g.intervene(in2)\n",
    "print(\"input_0 before intervention %s, after intervention %s\" % (before, after))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the `LOC` object to represent the indexing for an intervention location\n",
    "\n",
    "We can also specify where to intervene by passing an additional `locs` dictionary, with keys being node names, and  values being any of the following:\n",
    "\n",
    "1. `LOC[...]`. `LOC` is a special object that when applied the bracket indexing method `[]`, simply returns the underlying  object denoted by whatever is in the brackets.\n",
    "2. Indexing in a string form\n",
    "3. an `int` for single elements\n",
    "4. a tuple with `int`s, `slice` objects, or "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_0 before intervention [0], after intervention [0]\n",
      "input_0 before intervention [0], after intervention [0]\n",
      "input_0 before intervention [0], after intervention [0]\n",
      "input_0 before intervention [0], after intervention [0]\n",
      "input_0 before intervention [0], after intervention [0]\n"
     ]
    }
   ],
   "source": [
    "from intervention import LOC\n",
    "\n",
    "act3 = g.get_result(\"activation\", input_3)\n",
    "for i in range(0, 100, 20):\n",
    "    replace_value = act3[i:i+20]\n",
    "    inputs = {\"activation\": replace_value}\n",
    "    locs = {\"activation\": LOC[i:i+20]}\n",
    "    interv = Intervention(base=input_0, inputs=inputs, locs=locs)\n",
    "    \n",
    "    before, after = g.intervene(in2)\n",
    "    print(\"input_0 before intervention %s, after intervention %s\" % (before, after))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
